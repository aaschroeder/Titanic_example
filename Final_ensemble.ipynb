{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this section, we're going to\n",
    "\n",
    "   - Setup the titanic data for ensembling\n",
    "   - Call the five individual models\n",
    "   - Ensemble the models (We use another Gradient Booster\n",
    "   - Output for submission\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "titanic=pd.read_csv('./titanic_clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's normalize our continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_norm=['Age','Fare']\n",
    "col_norms=['Age_z','Fare_z']\n",
    "\n",
    "titanic[col_norms]=titanic[cols_to_norm].apply(lambda x: (x-x.mean())/x.std())\n",
    "\n",
    "titanic['cabin_clean']=(pd.notnull(titanic.Cabin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import  GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_target=titanic.Survived.values\n",
    "features=['Sex','SibSp','Parch','Pclass_1','Pclass_2','Pclass_3','Emb_C','Emb_Q','Emb_S',\\\n",
    "                         'Emb_nan','Age_ct_C','Age_ct_A','Age_ct_S', 'Sp_ct','Age_z','Fare_z',\\\n",
    "                        'Ti_Dr', 'Ti_Master', 'Ti_Mil', 'Ti_Miss', 'Ti_Mr', 'Ti_Mrs', 'Ti_Other', 'Ti_Rev',\\\n",
    "                         'Fl_AB', 'Fl_CD', 'Fl_EFG', 'Fl_nan']\n",
    "titanic_features=titanic[features].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_features, ensemble_features, titanic_target, ensemble_target= \\\n",
    "    train_test_split(titanic_features,\n",
    "                     titanic_target,\n",
    "                     test_size=.1,\n",
    "                     random_state=7132016)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In here will be the section where we import the relevant python code from each project\n",
    "\n",
    "Now, we'll import the three csv datasets, merge them on PassengerID, then ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rf_pred  gboost_pred  svm_pred  nb_pred  log_pred\n",
      "0        1            1         1        1         1\n",
      "1        1            1         1        1         1\n",
      "2        0            0         0        0         0\n",
      "3        1            0         1        1         0\n",
      "4        0            0         0        0         0\n"
     ]
    }
   ],
   "source": [
    "titanic_rf=pd.read_csv('./ensemble_rf.csv')\n",
    "titanic_gboost=pd.read_csv('./ensemble_gboost.csv')\n",
    "titanic_svm=pd.read_csv('./ensemble_svm.csv')\n",
    "titanic_nb=pd.read_csv('./ensemble_nb.csv')\n",
    "titanic_logit=pd.read_csv('./ensemble_logit.csv')\n",
    "\n",
    "\n",
    "titanic_ensemble=pd.merge(titanic_rf, titanic_gboost, left_index=True, right_index=True)\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_svm, left_index=True, right_index=True)\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_nb, left_index=True, right_index=True)\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_logit, left_index=True, right_index=True)\n",
    "\n",
    "print titanic_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the correlations are across the learners, to find how much similarity in information they share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              rf_pred  gboost_pred  svm_pred   nb_pred  log_pred\n",
      "rf_pred      1.000000     0.778462  0.635764  0.641416  0.744208\n",
      "gboost_pred  0.778462     1.000000  0.691861  0.540582  0.744208\n",
      "svm_pred     0.635764     0.691861  1.000000  0.568527  0.703526\n",
      "nb_pred      0.641416     0.540582  0.568527  1.000000  0.541967\n",
      "log_pred     0.744208     0.744208  0.703526  0.541967  1.000000\n"
     ]
    }
   ],
   "source": [
    "print titanic_ensemble.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the ensembling, using the same method for Gradient Boosting we used earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_ensemble=titanic_ensemble.values\n",
    "\n",
    "#clf=GradientBoostingClassifier().fit(titanic_ensemble,ensemble_target)\n",
    "\n",
    "feat_param=['deviance','exponential']\n",
    "score=0\n",
    "for feature in feat_param:\n",
    "    clf = GradientBoostingClassifier(loss=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5 )\n",
    "    if score_test.mean()>score:\n",
    "        loss_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "for feature in np.linspace(.05,.45,11):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5 )\n",
    "    if score_test.mean()>score:\n",
    "        rate_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "for feature in range(100,1001,100):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        feat_n_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "        \n",
    "score=0\n",
    "\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        depth_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        sample_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        sample_leaf_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "\n",
    "for feature in np.linspace(0.0,0.5,10):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5 )\n",
    "    if score_test.mean()>score:\n",
    "        frac_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "score=0\n",
    "\n",
    "for feature in np.linspace(0.1,1,10):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        subsamp_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "node_out=None\n",
    "\n",
    "for feature in range(2,11):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=subsamp_out, max_leaf_nodes=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_ensemble,ensemble_target,cv=5)\n",
    "    if score_test.mean()>score:\n",
    "        node_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "        \n",
    "clf=GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=subsamp_out, max_leaf_nodes=node_out,\\\n",
    "                                     random_state=7112016).fit(titanic_ensemble,ensemble_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_rf=pd.read_csv('./kaggle_titanic_submission_rf.csv')\n",
    "titanic_gboost=pd.read_csv('./kaggle_titanic_submission_gboost.csv')\n",
    "titanic_svm=pd.read_csv('./kaggle_titanic_submission_svm.csv')\n",
    "titanic_nb=pd.read_csv('./kaggle_titanic_submission_nb.csv')\n",
    "titanic_logit=pd.read_csv('./kaggle_titanic_submission_logit.csv')\n",
    "\n",
    "\n",
    "titanic_ensemble=pd.merge(titanic_rf, titanic_gboost, on='PassengerId')\n",
    "titanic_ensemble.rename(columns={'Survived_x':'rf_pred','Survived_y':'gboost_pred'}, inplace=True)\n",
    "\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_svm, on='PassengerId')\n",
    "titanic_ensemble.rename(columns={'Survived':'svm_pred'}, inplace=True)\n",
    "\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_nb, on='PassengerId')\n",
    "titanic_ensemble.rename(columns={'Survived':'nb_pred'}, inplace=True)\n",
    "\n",
    "titanic_ensemble=pd.merge(titanic_ensemble, titanic_logit, on='PassengerId')\n",
    "titanic_ensemble.rename(columns={'Survived':'log_pred'}, inplace=True)\n",
    "\n",
    "titanic_ensemble=titanic_ensemble[['rf_pred','gboost_pred','svm_pred', 'nb_pred','log_pred']]\n",
    "titanic_ensemble=titanic_ensemble.values\n",
    "\n",
    "predictions=clf.predict(titanic_ensemble)\n",
    "\n",
    "kaggle=pd.DataFrame({'PassengerId':titanic_rf['PassengerId']})\n",
    "kaggle['Survived']=predictions\n",
    "kaggle.to_csv('./kaggle_titanic_submission_ensemble.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
