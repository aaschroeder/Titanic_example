{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script implements a Gradient Boosting Machine on the Titanic dataset. This is a boosting algorithm that will be using trees, and will be auto-selecting features to evaluate. Since we're pretty agnostic about everything except prediction, let's just cycle through each of the option values to find some good ones (since we're not doing any backward evaluation after choosing an option, this probably isn't the optimal one, but it should be decent at least).\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "titanic=pd.read_csv('./titanic_clean_data.csv')\n",
    "\n",
    "cols_to_norm=['Age','Fare']\n",
    "col_norms=['Age_z','Fare_z']\n",
    "\n",
    "titanic[col_norms]=titanic[cols_to_norm].apply(lambda x: (x-x.mean())/x.std())\n",
    "\n",
    "titanic['cabin_clean']=(pd.notnull(titanic.Cabin))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import  GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "titanic_target=titanic.Survived.values\n",
    "features=['Sex','SibSp','Parch','Pclass_1','Pclass_2','Pclass_3','Emb_C','Emb_Q','Emb_S',\\\n",
    "                         'Emb_nan','Age_ct_C','Age_ct_A','Age_ct_S', 'Sp_ct','Age_z','Fare_z',\\\n",
    "                        'Ti_Dr', 'Ti_Master', 'Ti_Mil', 'Ti_Miss', 'Ti_Mr', 'Ti_Mrs', 'Ti_Other', 'Ti_Rev',\\\n",
    "                         'Fl_AB', 'Fl_CD', 'Fl_EFG', 'Fl_nan']\n",
    "titanic_features=titanic[features].values\n",
    "\n",
    "\n",
    "titanic_features, ensemble_features, titanic_target, ensemble_target= \\\n",
    "    train_test_split(titanic_features,\n",
    "                     titanic_target,\n",
    "                     test_size=.1,\n",
    "                     random_state=7132016)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we're going to do is go stepwise through many of the features of the random forest classifier, to figure out which parameters will give us the best fit. We'll go in the following order:\n",
    "\n",
    "   * loss\n",
    "   * learning_rate\n",
    "   * n_estimators\n",
    "   * max_depth (depth of tree)\n",
    "   * min_samples_split (number of samples you need to be able to create a new branch/node)\n",
    "   * min_samples_leaf\n",
    "   * min_weight_fraction_leaf\n",
    "   * subsample\n",
    "   * max_features (number of features considered at each split)\n",
    "   * max_leaf_nodes (implicitly, if the best is not none, then we'll be ignoring the max depth parameter\n",
    "   * warm_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.) Loss Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential\n"
     ]
    }
   ],
   "source": [
    "feat_param=['deviance','exponential']\n",
    "score=0\n",
    "for feature in feat_param:\n",
    "    clf = GradientBoostingClassifier(loss=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        loss_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "        \n",
    "print loss_out        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "for feature in np.linspace(.05,.45,11):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        rate_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print rate_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Count of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "for feature in range(100,1001,100):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        feat_n_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print feat_n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) Maximum depth of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        depth_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print depth_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.) The number of samples available in order to make another split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        sample_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print sample_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.) The number of samples that must appear in a leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "for feature in range(1,21):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        sample_leaf_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print sample_leaf_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.) Min required weighted fraction of samples in a leaf or node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "\n",
    "for feature in np.linspace(0.0,0.5,10):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        frac_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print frac_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.) Fraction of samples used in each base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "\n",
    "for feature in np.linspace(0.1,1,10):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        subsamp_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print subsamp_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.) Maximum possible number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "node_out=None\n",
    "\n",
    "for feature in range(2,11):\n",
    "    clf = GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=subsamp_out, max_leaf_nodes=feature, random_state=7112016)\n",
    "    score_test= cross_val_score(clf,titanic_features,titanic_target,cv=10 )\n",
    "    if score_test.mean()>score:\n",
    "        node_out=feature\n",
    "        score_diff=score_test.mean()-score\n",
    "        score=score_test.mean()\n",
    "\n",
    "print node_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(loss=loss_out, learning_rate=rate_out, n_estimators=feat_n_out,\\\n",
    "                                     max_depth=depth_out, min_samples_split=sample_out,\\\n",
    "                                     min_samples_leaf=sample_leaf_out, min_weight_fraction_leaf=frac_out,\\\n",
    "                                     subsample=subsamp_out, max_leaf_nodes=node_out,\\\n",
    "                                     random_state=7112016).fit(titanic_features, titanic_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('./test.csv')\n",
    "\n",
    "test_data.Sex.replace(['male','female'],[True,False], inplace=True)\n",
    "test_data.Age= test_data.groupby(['Sex','Pclass'])[['Age']].transform(lambda x: x.fillna(x.mean()))\n",
    "test_data.Fare= titanic.groupby(['Pclass'])[['Fare']].transform(lambda x: x.fillna(x.mean()))\n",
    "titanic_class=pd.get_dummies(test_data.Pclass,prefix='Pclass',dummy_na=False)\n",
    "test_data=pd.merge(test_data,titanic_class,on=test_data['PassengerId'])\n",
    "test_data=pd.merge(test_data,pd.get_dummies(test_data.Embarked, prefix='Emb', dummy_na=True), on=test_data['PassengerId'])\n",
    "titanic['Floor']=titanic['Cabin'].str.extract('^([A-Z])', expand=False)\n",
    "titanic['Floor'].replace(to_replace='T',value=np.NaN ,inplace=True)\n",
    "titanic=pd.merge(titanic,pd.get_dummies(titanic.Floor, prefix=\"Fl\", dummy_na=True),on=titanic['PassengerId'])\n",
    "test_data['Age_cut']=pd.cut(test_data['Age'],[0,17.9,64.9,99], labels=['C','A','S'])\n",
    "test_data=pd.merge(test_data,pd.get_dummies(test_data.Age_cut, prefix=\"Age_ct\", dummy_na=False),on=test_data['PassengerId'])\n",
    "\n",
    "test_data['Title']=test_data['Name'].str.extract(', (.*)\\.', expand=False)\n",
    "test_data['Title'].replace(to_replace='Mrs\\. .*',value='Mrs', inplace=True, regex=True)\n",
    "test_data.loc[test_data.Title.isin(['Col','Major','Capt']),['Title']]='Mil'\n",
    "test_data.loc[test_data.Title=='Mlle',['Title']]='Miss'\n",
    "test_data.loc[test_data.Title=='Mme',['Title']]='Mrs'\n",
    "test_data['Title_ct']=test_data.groupby(['Title'])['Title'].transform('count')\n",
    "test_data.loc[test_data.Title_ct<5,['Title']]='Other'\n",
    "test_data=pd.merge(test_data,pd.get_dummies(test_data.Title, prefix='Ti',dummy_na=False), on=test_data['PassengerId'])\n",
    "\n",
    "test_data['NameTest']=test_data.Name\n",
    "test_data['NameTest'].replace(to_replace=\" \\(.*\\)\",value=\"\",inplace=True, regex=True)\n",
    "test_data['NameTest'].replace(to_replace=\", M.*\\.\",value=\", \",inplace=True, regex=True)\n",
    "\n",
    "\n",
    "cols_to_norm=['Age','Fare']\n",
    "col_norms=['Age_z','Fare_z']\n",
    "\n",
    "test_data['Age_z']=(test_data.Age-titanic.Age.mean())/titanic.Age.std()\n",
    "test_data['Fare_z']=(test_data.Fare-titanic.Fare.mean())/titanic.Fare.std()\n",
    "\n",
    "\n",
    "test_data['cabin_clean']=(pd.notnull(test_data.Cabin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_list=pd.concat([titanic[['PassengerId','NameTest']],test_data[['PassengerId','NameTest']]])\n",
    "name_list['Sp_ct']=name_list.groupby('NameTest')['NameTest'].transform('count')-1\n",
    "test_data=pd.merge(test_data,name_list[['PassengerId','Sp_ct']],on='PassengerId',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_cols(var_check,df):\n",
    "\n",
    "    if var_check not in df.columns.values:\n",
    "        df[var_check]=0\n",
    "\n",
    "for x in features:\n",
    "    add_cols(x, test_data)\n",
    "    \n",
    "features=['Sex','SibSp','Parch','Pclass_1','Pclass_2','Pclass_3','Emb_C','Emb_Q','Emb_S',\\\n",
    "                         'Emb_nan','Age_ct_C','Age_ct_A','Age_ct_S', 'Sp_ct','Age_z','Fare_z',\\\n",
    "                        'Ti_Dr', 'Ti_Master', 'Ti_Mil', 'Ti_Miss', 'Ti_Mr', 'Ti_Mrs', 'Ti_Other', 'Ti_Rev',\\\n",
    "                         'Fl_AB', 'Fl_CD', 'Fl_EFG', 'Fl_nan']\n",
    "test_features=test_data[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(ensemble_features)\n",
    "ensemble_gboost=pd.DataFrame({'gboost_pred':predictions})\n",
    "ensemble_gboost.to_csv('./ensemble_gboost.csv', index=False)\n",
    "\n",
    "predictions=model.predict(test_features)\n",
    "test_data['Survived']=predictions\n",
    "kaggle=test_data[['PassengerId','Survived']]\n",
    "kaggle.to_csv('./kaggle_titanic_submission_gboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
